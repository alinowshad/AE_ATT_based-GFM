{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea53905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T16:59:40.203607Z",
     "iopub.status.busy": "2024-10-09T16:59:40.203150Z",
     "iopub.status.idle": "2024-10-09T16:59:53.793009Z",
     "shell.execute_reply": "2024-10-09T16:59:53.791852Z"
    },
    "papermill": {
     "duration": 13.608833,
     "end_time": "2024-10-09T16:59:53.795953",
     "exception": false,
     "start_time": "2024-10-09T16:59:40.187120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654573f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T16:59:53.828005Z",
     "iopub.status.busy": "2024-10-09T16:59:53.827558Z",
     "iopub.status.idle": "2024-10-09T17:00:05.838840Z",
     "shell.execute_reply": "2024-10-09T17:00:05.837605Z"
    },
    "papermill": {
     "duration": 12.030624,
     "end_time": "2024-10-09T17:00:05.841600",
     "exception": false,
     "start_time": "2024-10-09T16:59:53.810976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910808e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:05.874874Z",
     "iopub.status.busy": "2024-10-09T17:00:05.874431Z",
     "iopub.status.idle": "2024-10-09T17:00:17.331552Z",
     "shell.execute_reply": "2024-10-09T17:00:17.330172Z"
    },
    "papermill": {
     "duration": 11.476999,
     "end_time": "2024-10-09T17:00:17.334357",
     "exception": false,
     "start_time": "2024-10-09T17:00:05.857358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install rstl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2573d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:17.369611Z",
     "iopub.status.busy": "2024-10-09T17:00:17.368528Z",
     "iopub.status.idle": "2024-10-09T17:00:29.460971Z",
     "shell.execute_reply": "2024-10-09T17:00:29.459431Z"
    },
    "papermill": {
     "duration": 12.113723,
     "end_time": "2024-10-09T17:00:29.463973",
     "exception": false,
     "start_time": "2024-10-09T17:00:17.350250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63804ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:29.499222Z",
     "iopub.status.busy": "2024-10-09T17:00:29.498810Z",
     "iopub.status.idle": "2024-10-09T17:00:29.504218Z",
     "shell.execute_reply": "2024-10-09T17:00:29.502999Z"
    },
    "papermill": {
     "duration": 0.02602,
     "end_time": "2024-10-09T17:00:29.506435",
     "exception": false,
     "start_time": "2024-10-09T17:00:29.480415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install neuralprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c44fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:29.541117Z",
     "iopub.status.busy": "2024-10-09T17:00:29.540713Z",
     "iopub.status.idle": "2024-10-09T17:00:33.756212Z",
     "shell.execute_reply": "2024-10-09T17:00:33.755005Z"
    },
    "papermill": {
     "duration": 4.235784,
     "end_time": "2024-10-09T17:00:33.758818",
     "exception": false,
     "start_time": "2024-10-09T17:00:29.523034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c731694",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:33.793999Z",
     "iopub.status.busy": "2024-10-09T17:00:33.793425Z",
     "iopub.status.idle": "2024-10-09T17:00:43.736364Z",
     "shell.execute_reply": "2024-10-09T17:00:43.735075Z"
    },
    "papermill": {
     "duration": 9.963359,
     "end_time": "2024-10-09T17:00:43.738915",
     "exception": false,
     "start_time": "2024-10-09T17:00:33.775556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import Dense\n",
    "import gc\n",
    "from keras.layers import concatenate\n",
    "import csv\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "# import GPy, GPyOpt\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras\n",
    "from tcn import TCN\n",
    "\n",
    "from rstl import STL\n",
    "from texttable import Texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274c2acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:43.774186Z",
     "iopub.status.busy": "2024-10-09T17:00:43.773405Z",
     "iopub.status.idle": "2024-10-09T17:00:43.779727Z",
     "shell.execute_reply": "2024-10-09T17:00:43.778188Z"
    },
    "papermill": {
     "duration": 0.027076,
     "end_time": "2024-10-09T17:00:43.782526",
     "exception": false,
     "start_time": "2024-10-09T17:00:43.755450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004e208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:43.817509Z",
     "iopub.status.busy": "2024-10-09T17:00:43.817106Z",
     "iopub.status.idle": "2024-10-09T17:00:55.360759Z",
     "shell.execute_reply": "2024-10-09T17:00:55.359538Z"
    },
    "papermill": {
     "duration": 11.564457,
     "end_time": "2024-10-09T17:00:55.363557",
     "exception": false,
     "start_time": "2024-10-09T17:00:43.799100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b724a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:00:55.399282Z",
     "iopub.status.busy": "2024-10-09T17:00:55.398842Z",
     "iopub.status.idle": "2024-10-09T17:01:09.040647Z",
     "shell.execute_reply": "2024-10-09T17:01:09.039068Z"
    },
    "papermill": {
     "duration": 13.66313,
     "end_time": "2024-10-09T17:01:09.043442",
     "exception": false,
     "start_time": "2024-10-09T17:00:55.380312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install statsforecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0edef292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:09.082169Z",
     "iopub.status.busy": "2024-10-09T17:01:09.081726Z",
     "iopub.status.idle": "2024-10-09T17:01:11.549325Z",
     "shell.execute_reply": "2024-10-09T17:01:11.548062Z"
    },
    "papermill": {
     "duration": 2.490162,
     "end_time": "2024-10-09T17:01:11.552025",
     "exception": false,
     "start_time": "2024-10-09T17:01:09.061863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsforecast.models import AutoARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6e59abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:11.653049Z",
     "iopub.status.busy": "2024-10-09T17:01:11.652339Z",
     "iopub.status.idle": "2024-10-09T17:01:11.657730Z",
     "shell.execute_reply": "2024-10-09T17:01:11.656642Z"
    },
    "papermill": {
     "duration": 0.027512,
     "end_time": "2024-10-09T17:01:11.659961",
     "exception": false,
     "start_time": "2024-10-09T17:01:11.632449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import SARIMAX, ARIMA,ExponentialSmoothing, SimpleExpSmoothing, Holt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0c41b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:11.698625Z",
     "iopub.status.busy": "2024-10-09T17:01:11.698247Z",
     "iopub.status.idle": "2024-10-09T17:01:11.703220Z",
     "shell.execute_reply": "2024-10-09T17:01:11.702090Z"
    },
    "papermill": {
     "duration": 0.027379,
     "end_time": "2024-10-09T17:01:11.705881",
     "exception": false,
     "start_time": "2024-10-09T17:01:11.678502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# raw_data = pd.read_csv(\"/kaggle/input/nn5-dataset/Fixed_nn5.csv\",_=',', header=None) # Hospital - Horizon 12\n",
    "# features = pd.read_csv(\"/kaggle/input/nn5-featureset-autoformer/Features_nn5_Autoformer_max_8_label_mv41_raw.csv\",sep=',', header=None) # CIF 2016 - Horizon 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a6a39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:11.744935Z",
     "iopub.status.busy": "2024-10-09T17:01:11.744217Z",
     "iopub.status.idle": "2024-10-09T17:01:11.749064Z",
     "shell.execute_reply": "2024-10-09T17:01:11.747949Z"
    },
    "papermill": {
     "duration": 0.026963,
     "end_time": "2024-10-09T17:01:11.751298",
     "exception": false,
     "start_time": "2024-10-09T17:01:11.724335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(raw_data.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abb03c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:11.790397Z",
     "iopub.status.busy": "2024-10-09T17:01:11.789894Z",
     "iopub.status.idle": "2024-10-09T17:01:12.088461Z",
     "shell.execute_reply": "2024-10-09T17:01:12.087175Z"
    },
    "papermill": {
     "duration": 0.321519,
     "end_time": "2024-10-09T17:01:12.091186",
     "exception": false,
     "start_time": "2024-10-09T17:01:11.769667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rd='Mahdian/'\n",
    "\n",
    "def get_dataset_params(dataset_name = 'cif-12'):\n",
    "    suilin_smape = False\n",
    "\n",
    "    #-------------------------------------------------- CIF 2016 ------------------------------------------------#\n",
    "    if dataset_name == 'cif-6':\n",
    "        dataset_path = '/kaggle/working'+'/'+'CIF2016'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/cifnewdataset/6.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "#         features = pd.read_csv(\"/kaggle/input/ae-cif012-unsupervised-ae/Features_LSTM_cif012_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "#         features = pd.read_csv('/kaggle/input/cif-data-and-hynd-khs-feature/fs_hyndman_freqfind_cif12.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 15\n",
    "        look_forward = 6\n",
    "        batch_size = 3\n",
    "        epochs = 30\n",
    "        learning_rate = 0.0001\n",
    "        features=raw_data\n",
    "\n",
    "        suilin_smape = False\n",
    "        frequency = None\n",
    "\n",
    "    elif dataset_name == 'cif-12':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'CIF2016'\n",
    "\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/cifnewdataset/12.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "#         features = pd.read_csv(\"/kaggle/input/ae-cif012-unsupervised-ae/Features_LSTM_cif012_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "        features = pd.read_csv('/kaggle/input/cif-data-and-hynd-khs-feature/fs_khs_CIF-12_with_find_freq.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "#         raw_data = raw_data.iloc[:, :-6]\n",
    "        lag = 36\n",
    "        look_forward = 12\n",
    "        batch_size = 6\n",
    "        epochs = 25\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        suilin_smape = False\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "\n",
    "\n",
    "    #-------------------------------------------------- Hospital ------------------------------------------------#\n",
    "    #-------------------------------------------------- Hospital ------------------------------------------------#\n",
    "\n",
    "    elif dataset_name == 'tourism':\n",
    "        dataset_path = rd+'/'+'tourism'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/tourism/Tourism-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "        features = pd.read_csv('/kaggle/input/tourism/fs_KHS_Tourism.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "#         features = pd.read_csv('/kaggle/input/hospital-tourims-attention-feat/Features_tourism_attenLSTM_8.csv',sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "\n",
    "        lag = 20\n",
    "        look_forward = 8\n",
    "        batch_size = 100\n",
    "        epochs =50\n",
    "        learning_rate = 0.001\n",
    "        suilin_smape = False\n",
    "        frequency = 4\n",
    "        # frequency = None      \n",
    "    elif dataset_name == 'hospital':\n",
    "        dataset_path = rd+'/'+'Hospital'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/Hospital_new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "#         raw_data = pd.read_csv(\"/kaggle/input/hospital-dataset/hospital_dataset.txt\",sep='delimeter', header=None) # Hospital - Horizon 12\n",
    "        features = pd.read_csv(\"/kaggle/input/hospital-dataset/fs_khs_hopsiptal_findfreq.csv\",sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "#         features = pd.read_csv(\"/kaggle/input/ae-hospital-unsupervised-ae/Features_hospital_LSTM_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 30\n",
    "        look_forward = 12\n",
    "        batch_size = 20\n",
    "        epochs = 50\n",
    "        learning_rate = 0.0001\n",
    "#         frequency = 12\n",
    "        frequency = None\n",
    "    elif dataset_name == 'nn5':\n",
    "        dataset_path = rd+'/'+'nn5'\n",
    "        raw_data = pd.read_csv(\"/kaggle/input/nn5-dataset/nn51.csv\",sep=',', header=None) # Hospital - Horizon 12\n",
    "#         features = pd.read_csv(\"/kaggle/input/nn5-dataset/fs_hyndman_Fixed_nn5.csv\",sep=',', header=0) # CIF 2016 - Horizo/kaggle/input/nn5-featureset-autoformer/Features_nn5_Autoformer_max_8_label_mv41_raw.csvn 6\n",
    "        features = pd.read_csv(\"/kaggle/input/nn5-hyndman-khs/fs_KHS_NN5 (2).csv\",sep=',', header=0) # CIF 2016 - Horizo/kaggle/input/nn5-featureset-autoformer/Features_nn5_Autoformer_max_8_label_mv41_raw.csvn 6\n",
    "\n",
    "        #Features_nn5_Autoformer_max_mean_dim8_head8_mv41_25_stride5.\n",
    "        lag = 150\n",
    "        look_forward = 56\n",
    "        batch_size = 20\n",
    "        epochs = 50\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 7\n",
    "        # frequency = None\n",
    "\n",
    "    #---------------------------------------------- Kaggle Web ---------------------------------------------------#\n",
    "\n",
    "#     elif dataset_name == 'kaggle':\n",
    "#         dataset_path = rd+'/'+'Kaggle'\n",
    "#         raw_data = pd.read_csv(\"/kaggle/input/kaggle/kaggle_web_traffic_dataset.txt\",sep='delimeter', header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "#         features = pd.read_csv(\"/kaggle/input/kaggle/kaggle_web_traffic_dataset-hyndman-features.csv\",sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "# #         features = pd.read_csv(\"/kaggle/input/one-ae-kaggle-unsupervised-ae/Features_kaggle_8.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "#         lag = 400\n",
    "#         look_forward = 59\n",
    "#         batch_size = 70\n",
    "#         epochs = 20\n",
    "#         learning_rate = 0.0001\n",
    "#         suilin_smape = True\n",
    "#         frequency = 30\n",
    "        # frequency = None\n",
    "\n",
    "    #--------------------------------------------------M3 Monthly------------------------------------------------#\n",
    "    elif dataset_name == 'm3-demo':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/m3-att-lstm/m3-demo-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "        features = pd.read_csv(\"/kaggle/input/m3-hyndman-khs-features/fs_KHS_m3_demo.csv\", sep=',', header=0)\n",
    "#         features = pd.read_csv(\"/kaggle/input/m3-att-lstm/M3-attCNN/Features_demo_32.csv\",sep=',', header=None)\n",
    "        \n",
    "        lag = 29\n",
    "        look_forward = 18\n",
    "        batch_size = 7\n",
    "        epochs = 20\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-finance':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/m3-att-lstm/m3-finance-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        features = pd.read_csv(\"/kaggle/input/m3-hyndman-khs-features/fs_KHS_m3_finance.csv\",sep=',', header=0)\n",
    "#         raw_data = raw_data.iloc[:, :-9]\n",
    "#         features = pd.read_csv(\"/kaggle/input/m3-att-lstm/M3-attCNN/Features_finance_8.csv\",sep=',', header=None)\n",
    "        lag = 27\n",
    "        look_forward = 18#18\n",
    "        batch_size = 9\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-industry':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/m3-att-lstm/m3-industry-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #note\n",
    "        features = pd.read_csv(\"/kaggle/input/m3-hyndman-khs-features/fs_KHS_m3_industry.csv\",sep=',', header=0)\n",
    "#         features = pd.read_csv(\"/kaggle/input/m3-att-lstm/M3-attCNN/Features_industry_32.csv\",sep=',', header=None)\n",
    "\n",
    "        lag = 26\n",
    "        look_forward = 18\n",
    "        batch_size = 10\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-macro':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/m3-att-lstm/m3-macro-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "#         features = pd.read_csv(\"/kaggle/input/m3-att-lstm/M3-attCNN/Features_macro_32.csv\",sep=',', header=None)\n",
    "        features = pd.read_csv(\"/kaggle/input/m3-hyndman-khs-features/fs_KHS_m3_macro.csv\",sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 26\n",
    "        look_forward = 18\n",
    "        batch_size = 15\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-micro':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/m3-att-lstm/M3-micro-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "        features = pd.read_csv(\"/kaggle/input/m3-hyndman-khs-features/fs_KHS_m3_micro.csv\",sep=',', header=0)\n",
    "#         features = pd.read_csv(\"/kaggle/input/m3-hyndman-khs-features/m3_micro-hyndman-features.csv\",sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 18\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "#         frequency = 12\n",
    "        frequency = None\n",
    "# \n",
    "    elif dataset_name == 'm3-other':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/m3-att-lstm/m3-other-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "#         features = pd.read_csv(\"/kaggle/input/m3-att-lstm/M3-attCNN/Features_other_32.csv\",sep=',', header=None)\n",
    "        features = pd.read_csv(\"/kaggle/input/m3-hyndman-khs-features/fs_KHS_m3_other.csv\",sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 26\n",
    "        look_forward = 18\n",
    "        batch_size = 2\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    sample_overlap = look_forward - 1\n",
    "\n",
    "    raw_data = raw_data.to_numpy().astype('float64')\n",
    "    features = features.to_numpy().astype('float64')\n",
    "    dataset = []\n",
    "    seri_len=[]\n",
    "    for i in range(len(raw_data)):\n",
    "        dataset.append(raw_data[i][~np.isnan(raw_data[i])])\n",
    "        seri_len.append(len(raw_data[i][~np.isnan(raw_data[i])]))\n",
    "\n",
    "\n",
    "    print(dataset_name,np.min(seri_len),np.max(seri_len))\n",
    "        \n",
    "\n",
    "    return dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d38323cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.129720Z",
     "iopub.status.busy": "2024-10-09T17:01:12.129322Z",
     "iopub.status.idle": "2024-10-09T17:01:12.151308Z",
     "shell.execute_reply": "2024-10-09T17:01:12.150063Z"
    },
    "papermill": {
     "duration": 0.044637,
     "end_time": "2024-10-09T17:01:12.154077",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.109440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, look_forward ):\n",
    "    data_means = [];\n",
    "    for index in range(len(dataset)):\n",
    "    # Mean Noramlization\n",
    "        series_mean = np.mean(dataset[index][:len(dataset[index]) - look_forward]) # Train Mean: look_forward || Full Mean: Mean: look_forward = 0\n",
    "\n",
    "        if series_mean == 0:\n",
    "            series_mean = 0.001\n",
    "\n",
    "        data_means.append(series_mean)\n",
    "        dataset[index] = np.divide(dataset[index], series_mean)\n",
    "        if np.min(dataset[index][:len(dataset[index])])<=0:\n",
    "                    dataset[index] = np.log(dataset[index] + 1)\n",
    "        else:\n",
    "             dataset[index] = np.log(dataset[index])\n",
    "        # Log Transformation\n",
    "#         dataset[index] = np.log(dataset[index] + 1)\n",
    "       \n",
    "\n",
    "\n",
    "    return dataset, np.array(data_means)\n",
    "\n",
    "def rescale_data_to_main_value(data, means, dataset_seasonal = []):\n",
    "    for index in range(len(data)):\n",
    "        if len(dataset_seasonal) != 0:\n",
    "            data[index] = data[index] + dataset_seasonal[index]\n",
    "        # Revert Log Transformation\n",
    "        data[index] = np.e ** data[index]\n",
    "#         data[index] = data[index]\n",
    "\n",
    "        # Revert Mean Normalization\n",
    "        data[index] = means[index] * data[index]\n",
    "    \n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_feature_vectors(features):\n",
    "    #------------------- Z-score ----------------------#\n",
    "#     means = features.mean(0)\n",
    "#     stds = features.std(0)\n",
    "\n",
    "#     for i in range(len(features)):\n",
    "#         features[i] = (features[i] - means) / stds\n",
    "\n",
    "    #--------------------Min - Max---------------------#\n",
    "    minimum = features.min(0)\n",
    "    maximum = features.max(0)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        features[i] = (features[i] - minimum) / (maximum - minimum)\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\"\"\"![root_mean_square_deviation.svg](attachment:root_mean_square_deviation.svg)\"\"\"\n",
    "\n",
    "#RMSE\n",
    "def root_mean_squared_error(actual, forecast, method = 'single_value'):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "\n",
    "        return np.sqrt(np.mean(np.square(actual - forecast)))\n",
    "    elif method == 'per_series':\n",
    "        rmses = []\n",
    "        for i in range(len(actual)):\n",
    "            rmses.append(np.sqrt(np.mean(np.square(actual[i] - forecast[i]))))\n",
    "\n",
    "        return rmses\n",
    "\n",
    "\"\"\"![YIy33.png](attachment:YIy33.png)\"\"\"\n",
    "\n",
    "#SMAPE\n",
    "def single_point_smape(actual, forecast, suilin_smape = False):\n",
    "    if suilin_smape == True:\n",
    "        epsilon = 0.1\n",
    "\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / max((np.abs(actual) + np.abs(forecast))+ epsilon, 0.5 + epsilon)))\n",
    "    else:\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))))\n",
    "\n",
    "def smape(actual, forecast, method = 'single_value', suilin_smape = False):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "        sum_smape = 0\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape += single_point_smape(actual[i], forecast[i], suilin_smape)\n",
    "        return 100 * sum_smape / len(actual)\n",
    "\n",
    "    elif method == 'per_series':\n",
    "        smapes = []\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape = 0\n",
    "            for j in range(len(actual[i])):\n",
    "                sum_smape += single_point_smape(actual[i,j], forecast[i,j], suilin_smape)\n",
    "            smapes.append(100 * sum_smape / len(actual[i]))\n",
    "        return np.array(smapes)\n",
    "\n",
    "# Create Samples from DataSet\n",
    "def create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "\n",
    "    dataX, dataY, dataY_seasonal = [], [], []\n",
    "    dataX_means, dataY_means = [], []\n",
    "    for i in range(0, len(sample) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        dataX.append(sample[i:(i+look_back), 0])\n",
    "        dataY.append(sample[(i + look_back):(i + look_back + look_forward), 0])\n",
    "\n",
    "        dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "    return np.array(dataX), np.array(dataY), np.array(dataY_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f4287b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.193867Z",
     "iopub.status.busy": "2024-10-09T17:01:12.192633Z",
     "iopub.status.idle": "2024-10-09T17:01:12.205578Z",
     "shell.execute_reply": "2024-10-09T17:01:12.204171Z"
    },
    "papermill": {
     "duration": 0.035074,
     "end_time": "2024-10-09T17:01:12.207820",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.172746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset3_arima(sample, look_back, look_forward, sample_overlap, dataset_seasonal,dataset_name):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    print('dddd')\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "    # print(\"sample.shape\",sample.shape)\n",
    "    sample_trn=sample[0:len(sample)-look_forward]\n",
    "    frequency=12\n",
    "    if dataset_name=='tourism':\n",
    "        frequency=4\n",
    "    if dataset_name=='cif-6':\n",
    "        frequency=12\n",
    "#     print(\"freq\",frequency)\n",
    "    #         fit1 = pm.auto_arima(sample_trn,trace=True,error_action=\"ignore\",stepwise=False,seasonal=True,m=frequency,information_criterion='aic')#,stepwise=True,information_criterion='aic')\n",
    "\n",
    "    if len(sample_trn) > frequency*2:\n",
    "        \n",
    "        fit1 =pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5,n_jobs=-1)\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "           \n",
    "           \n",
    "    elif len(sample_trn)<frequency*2 and len(sample_trn)>frequency :\n",
    "        frequency = int(frequency/2)\n",
    "        fit1 = pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5)\n",
    "\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "    else:\n",
    "        fit1 = pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5)\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "\n",
    "\n",
    "#     fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=12, trend='add',\n",
    "#                                 seasonal='add').fit()\n",
    "#     aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    # print(aug_seri)\n",
    "    from_train = sample_trn[-(look_back+look_forward-1):]\n",
    "    frm_train_aug=np.concatenate([from_train.reshape(-1,1),aug_seri.reshape(-1,1)])\n",
    "    frm_train_aug=frm_train_aug.flatten()\n",
    "    # print(frm_train_aug)\n",
    "    # print(from_train[:,0])\n",
    "\n",
    "\n",
    "    aug_trainX,aug_trainY=[],[]\n",
    "    for i in range(0, len(frm_train_aug) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        aug_trainX.append(frm_train_aug[i:(i+look_back)])\n",
    "        aug_trainY.append(frm_train_aug[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "    return np.array(aug_trainX), np.array(aug_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14473328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.247970Z",
     "iopub.status.busy": "2024-10-09T17:01:12.247500Z",
     "iopub.status.idle": "2024-10-09T17:01:12.259986Z",
     "shell.execute_reply": "2024-10-09T17:01:12.258670Z"
    },
    "papermill": {
     "duration": 0.036245,
     "end_time": "2024-10-09T17:01:12.262424",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.226179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset2(sample, look_back, look_forward, sample_overlap, dataset_seasonal,dataset_name):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "    # print(\"sample.shape\",sample.shape)\n",
    "    sample_trn=sample[0:len(sample)-look_forward]\n",
    "    \n",
    "    frequency=12\n",
    "    if dataset_name=='tourism':\n",
    "        frequency=4\n",
    "    if dataset_name=='cif-6':\n",
    "        frequency=12\n",
    "#     print(\"freq\",frequency)\n",
    "    \n",
    "    if len(sample_trn) > frequency*2:\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=frequency, trend='add',\n",
    "                            seasonal='add').fit()\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "           \n",
    "           \n",
    "    elif len(sample_trn)<frequency*2 and len(sample_trn)>frequency :\n",
    "        frequency = int(frequency/2)\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=frequency, trend='add',\n",
    "                            seasonal='add').fit()\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    else:\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten())).fit()\n",
    "\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "\n",
    "#     fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=12, trend='add',\n",
    "#                                 seasonal='add').fit()\n",
    "#     aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    # print(aug_seri)\n",
    "    from_train = sample_trn[-(look_back+look_forward-1):]\n",
    "    frm_train_aug=np.concatenate([from_train.reshape(-1,1),aug_seri.reshape(-1,1)])\n",
    "    frm_train_aug=frm_train_aug.flatten()\n",
    "    # print(frm_train_aug)\n",
    "    # print(from_train[:,0])\n",
    "\n",
    "\n",
    "    aug_trainX,aug_trainY=[],[]\n",
    "    for i in range(0, len(frm_train_aug) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        aug_trainX.append(frm_train_aug[i:(i+look_back)])\n",
    "        aug_trainY.append(frm_train_aug[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "    return np.array(aug_trainX), np.array(aug_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64cca6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.301964Z",
     "iopub.status.busy": "2024-10-09T17:01:12.301003Z",
     "iopub.status.idle": "2024-10-09T17:01:12.309494Z",
     "shell.execute_reply": "2024-10-09T17:01:12.308268Z"
    },
    "papermill": {
     "duration": 0.030655,
     "end_time": "2024-10-09T17:01:12.311930",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.281275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stl_decomposition2(dataset, frequency,look):\n",
    "    seasonal = []\n",
    "    trend = []\n",
    "    for index in range(len(dataset)):\n",
    "        if frequency != None:\n",
    "            stl = STL(dataset[index][:len(dataset[index]) - look], frequency, \"periodic\")\n",
    "            stl_tot = STL(dataset[index], frequency, \"periodic\")\n",
    "\n",
    "            seasonal.append(np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]]))\n",
    "            trend.append(np.concatenate([stl.trend,stl_tot.trend[-look:]]))\n",
    "            # deseason_part=dataset[index][:len(dataset[index]) - look] - stl.seasonal\n",
    "            # d=np.concatenate([deseason_part,dataset[index][-look:]])\n",
    "            dataset[index]=dataset[index]-np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]])\n",
    "        else:\n",
    "            seasonal.append(np.zeros((dataset[index].shape)))\n",
    "            trend.append(np.zeros((dataset[index].shape)))\n",
    "\n",
    "    return dataset, np.array(seasonal), np.array(trend)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eda2eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.350705Z",
     "iopub.status.busy": "2024-10-09T17:01:12.349739Z",
     "iopub.status.idle": "2024-10-09T17:01:12.354687Z",
     "shell.execute_reply": "2024-10-09T17:01:12.353462Z"
    },
    "papermill": {
     "duration": 0.027,
     "end_time": "2024-10-09T17:01:12.357257",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.330257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NOTE Frequency for MASE computing when no deseasonalization is happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b7e1cd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.396455Z",
     "iopub.status.busy": "2024-10-09T17:01:12.396069Z",
     "iopub.status.idle": "2024-10-09T17:01:12.404447Z",
     "shell.execute_reply": "2024-10-09T17:01:12.403276Z"
    },
    "papermill": {
     "duration": 0.030532,
     "end_time": "2024-10-09T17:01:12.406611",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.376079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_point_mase(actual, forecast, insample, frequency) :\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    # print(\"HHHHHHHHHHHHHHHHHHHHHHHHHHHH\")\n",
    "    return np.mean(np.abs(forecast - actual)) / np.mean(np.abs(insample[:-frequency] - insample[frequency:]))\n",
    "\n",
    "\n",
    "def mase(actual, forecast, insample,frequency):\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f89d51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.444615Z",
     "iopub.status.busy": "2024-10-09T17:01:12.444241Z",
     "iopub.status.idle": "2024-10-09T17:01:12.451596Z",
     "shell.execute_reply": "2024-10-09T17:01:12.450531Z"
    },
    "papermill": {
     "duration": 0.029028,
     "end_time": "2024-10-09T17:01:12.453944",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.424916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mase_val(actual, forecast, insample,frequency):\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-2*len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c557a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.493005Z",
     "iopub.status.busy": "2024-10-09T17:01:12.492589Z",
     "iopub.status.idle": "2024-10-09T17:01:12.515868Z",
     "shell.execute_reply": "2024-10-09T17:01:12.514722Z"
    },
    "papermill": {
     "duration": 0.04617,
     "end_time": "2024-10-09T17:01:12.518521",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.472351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample(look_forward,sample_seasonal,dataX, dataY, data_mean, dataY_seasonal,frequency):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "    \n",
    "    # print(frequency)\n",
    "    # print(sample_seasonal)\n",
    "    # test_size=int(len(dataX) * testsize)\n",
    "    # val_size=int((len(dataX) - test_size) * valsize)\n",
    "#     print(dataY)\n",
    "    train_size0=(len(dataX)-test_size)-look_forward+1\n",
    "    train_size=(len(dataX)-test_size)\n",
    "#     print(\"train_size\",train_size)\n",
    "\n",
    "#     trainX, testX = dataX[0:train_size0,:], dataX[train_size:,:]  #\n",
    "#     trainY, testY = dataY[0:train_size0,:], dataY[train_size:,:]\n",
    "\n",
    "\n",
    "    trainX, testX = dataX[0:train_size0-val_size,:], dataX[train_size:,:]  #note -valsize is added now\n",
    "    trainY, testY = dataY[0:train_size0-val_size,:], dataY[train_size:,:]\n",
    "\n",
    "    valX, valY = dataX[train_size0-val_size:train_size0,:],dataY[train_size0-val_size:train_size0, :]\n",
    "#     print(\"valY\",valY)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size0-val_size:train_size0, :]\n",
    "    # print(\"len val\",len(val_seasonal))\n",
    "    # print(\"val\",val_seasonal)\n",
    "\n",
    "    train=dataY_seasonal[:train_size0, :]\n",
    "    train=train.reshape(-1,1)\n",
    "#     train2=train[:len(train):len(valY[0])]\n",
    "    # print(train2.reshape(1,-1))\n",
    "    # print(len(train2))\n",
    "\n",
    "    test2 = []\n",
    "\n",
    "\n",
    "    # modl = pm.auto_arima(train2, trace=False,seasonal=True, stepwise=False, information_criterion='aic',)\n",
    "    # preds = modl.predict(n_periods=len(valY[0]))\n",
    "    # print(\"xx\",train2.shape)\n",
    "    sample_size = len(sample_seasonal.flatten()) - look_forward\n",
    "\n",
    "    train3=sample_seasonal[:sample_size].flatten()\n",
    "#     print(\"len,frequency\",len(train3.flatten()),frequency)\n",
    "    if frequency!=None:\n",
    "        if len(train3.flatten()) > frequency*2:\n",
    "            sp = frequency\n",
    "           \n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp,trend='add', seasonal='add').fit()\n",
    "\n",
    "        elif len(train3.flatten())<frequency*2 and len(train3.flatten())>frequency :\n",
    "            sp = int(frequency/2)\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp, trend='add',\n",
    "                                        seasonal='add').fit()\n",
    "        else:\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten())).fit()\n",
    "\n",
    "        preds2 = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "        # print(\"preds\",preds2,type(preds2))\n",
    "    else:\n",
    "        preds2=np.zeros(look_forward)\n",
    "#     for i in range(0,len(val_seasonal[0])):\n",
    "#         test2.append(val_seasonal[0][len(val_seasonal[0])-1])\n",
    "\n",
    "    # print(\"datay_seasonal\",dataY_seasonal)\n",
    "\n",
    "    test_seasonal_y = dataY_seasonal[train_size:,:]\n",
    "    # print(\"test\",test_seasonal_y)\n",
    "    # print(\"train2\",train2.flatten())\n",
    "    # np.savetxt('train.csv', train2.flatten(), delimiter=', ')\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal,test_seasonal_y, preds2\n",
    "\n",
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "\n",
    "    \n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "        #for adding augmented data\n",
    "#         trainX = trainX + aug_trainX.tolist()\n",
    "#         trainY = trainY + aug_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)\n",
    "\n",
    "def save_prediction_result(data, dataset_name = 'cif-6', dataset_path = ''):\n",
    "    if dataset_name == '':\n",
    "        filename = dataset_name + '-results.csv'\n",
    "    else:\n",
    "        filename = dataset_path + '/' + dataset_name + '-results.csv'\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, sep=',',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "530186cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.556329Z",
     "iopub.status.busy": "2024-10-09T17:01:12.555931Z",
     "iopub.status.idle": "2024-10-09T17:01:12.568099Z",
     "shell.execute_reply": "2024-10-09T17:01:12.566993Z"
    },
    "papermill": {
     "duration": 0.03409,
     "end_time": "2024-10-09T17:01:12.570830",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.536740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def all_pre_process_aug(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency,dataset_name):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    " \n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "        aug_trainX,aug_trainY = create_dataset2(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index],dataset_name)\n",
    "#         aug_trainX,aug_trainY = create_dataset3_arima(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index],dataset_name)\n",
    "\n",
    "        aug_trainX= np.reshape(aug_trainX, (aug_trainX.shape[0],1, aug_trainX.shape[1]))\n",
    "    \n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "        # print(\"temp_trainX.shape\",temp_trainX.shape)\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "        #for adding augmented data\n",
    "        trainX = trainX + aug_trainX.tolist()\n",
    "        trainY = trainY + aug_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95024336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.609240Z",
     "iopub.status.busy": "2024-10-09T17:01:12.608872Z",
     "iopub.status.idle": "2024-10-09T17:01:12.622793Z",
     "shell.execute_reply": "2024-10-09T17:01:12.621564Z"
    },
    "papermill": {
     "duration": 0.035767,
     "end_time": "2024-10-09T17:01:12.625075",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.589308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample_sep_val(look_forward,sample_seasonal,dataX, dataY, data_mean, dataY_seasonal,frequency):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "    # print(frequency)\n",
    "    # print(sample_seasonal)\n",
    "    # test_size=int(len(dataX) * testsize)\n",
    "    # val_size=int((len(dataX) - test_size) * valsize)\n",
    "#     print(\"LENNNN\",len(dataX))\n",
    "    train_size=(len(dataX)-test_size-val_size)\n",
    "#     print(\"trainsize, train+val\",train_size,train_size +val_size)\n",
    "\n",
    "    trainX, testX = dataX[0:train_size,:], dataX[train_size+val_size:,:]\n",
    "    trainY, testY = dataY[0:train_size,:], dataY[train_size+val_size:,:]\n",
    "\n",
    "    valX, valY = dataX[train_size:train_size +val_size,:],dataY[train_size:train_size+val_size, :]\n",
    "#     print(\"LENNNN\",len(valX),len(valY),valX.shape,valY.shape)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size:train_size+val_size, :]\n",
    "    # print(\"len val\",len(val_seasonal))\n",
    "#     print(\"valseasonal \",val_seasonal.shape)\n",
    "\n",
    "#     train=dataY_seasonal[:train_size, :]\n",
    "#     train=train.reshape(-1,1)\n",
    "   \n",
    "    # print(train2.reshape(1,-1))\n",
    "    # print(len(train2))\n",
    "\n",
    "#     test2 = []\n",
    "\n",
    "\n",
    "    # modl = pm.auto_arima(train2, trace=False,seasonal=True, stepwise=False, information_criterion='aic',)\n",
    "    # preds = modl.predict(n_periods=len(valY[0]))\n",
    "    # print(\"xx\",train2.shape)\n",
    "    sample_size = len(sample_seasonal.flatten()) - look_forward\n",
    "\n",
    "    train3=sample_seasonal[:sample_size].flatten()\n",
    "\n",
    "    if frequency!=None:\n",
    "        if len(train3.flatten()) > frequency*2:\n",
    "            sp = frequency\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp,trend='add', seasonal='add').fit()\n",
    "\n",
    "        elif len(train3.flatten())<frequency*2 and len(train3.flatten())>frequency :\n",
    "            sp = int(frequency/2)\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp, trend='add',\n",
    "                                        seasonal='add').fit()\n",
    "        else:\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten())).fit()\n",
    "\n",
    "        preds2 = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "        # print(\"preds\",preds2,type(preds2))\n",
    "    else:\n",
    "        preds2=np.zeros(look_forward)\n",
    "#     for i in range(0,len(val_seasonal[0])):\n",
    "#         test2.append(val_seasonal[0][len(val_seasonal[0])-1])\n",
    "\n",
    "    # print(\"datay_seasonal\",dataY_seasonal)\n",
    "\n",
    "    test_seasonal_y = dataY_seasonal[train_size+val_size:,:]\n",
    "    \n",
    "#     print(\"test_seasonal\",test_seasonal_y.shape)\n",
    "    # print(\"train2\",train2.flatten())\n",
    "    # np.savetxt('train.csv', train2.flatten(), delimiter=', ')\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal,test_seasonal_y, preds2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2069f6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.663871Z",
     "iopub.status.busy": "2024-10-09T17:01:12.663025Z",
     "iopub.status.idle": "2024-10-09T17:01:12.668372Z",
     "shell.execute_reply": "2024-10-09T17:01:12.667118Z"
    },
    "papermill": {
     "duration": 0.02732,
     "end_time": "2024-10-09T17:01:12.670792",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.643472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fae8f628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.709923Z",
     "iopub.status.busy": "2024-10-09T17:01:12.709528Z",
     "iopub.status.idle": "2024-10-09T17:01:12.718094Z",
     "shell.execute_reply": "2024-10-09T17:01:12.717071Z"
    },
    "papermill": {
     "duration": 0.031476,
     "end_time": "2024-10-09T17:01:12.720799",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.689323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def creat_model_lstm(lag,dense_neuron,look_forward):\n",
    "    print('LSTM')\n",
    "    denselayer_activation = 'relu' #None\n",
    "    output_activation = 'linear' #'linear'\n",
    "    input_layer = layers.Input(shape = (1, lag,), name = \"Input-Layer\")\n",
    "\n",
    "    lstm1=keras.layers.LSTM(16,activation='relu',return_sequences=True)(input_layer)\n",
    "    lstm2=keras.layers.LSTM(8,activation='relu',return_sequences=True)(lstm1)\n",
    "    conv = keras.layers.Conv1D(4,\n",
    "                                  strides=2,\n",
    "                                  kernel_size=4,\n",
    "                                  activation=None,\n",
    "                                  padding=\"same\",)(lstm2)\n",
    "\n",
    "    flatten_layer2=keras.layers.Flatten(name=\"Flatten-Layer2\")(conv)\n",
    "\n",
    "    dense_layer1 = Dense(\n",
    "        look_forward,\n",
    "        activation = denselayer_activation, \n",
    "   \n",
    "\n",
    "        name = \"Fully-Connected-Layer\")(flatten_layer2)\n",
    "\n",
    "    dense_layer2 = Dense(\n",
    "        look_forward,\n",
    "        activation = None,\n",
    "        name = \"Output-Layer\")(dense_layer1)\n",
    "\n",
    "    # Create Model\n",
    "    model = Model(inputs = [input_layer], outputs = dense_layer2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d724748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.759430Z",
     "iopub.status.busy": "2024-10-09T17:01:12.758972Z",
     "iopub.status.idle": "2024-10-09T17:01:12.767426Z",
     "shell.execute_reply": "2024-10-09T17:01:12.766237Z"
    },
    "papermill": {
     "duration": 0.030515,
     "end_time": "2024-10-09T17:01:12.769709",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.739194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def creat_model_TFR(lag,dense_neuron,look_forward):\n",
    "    dense_neuron=50\n",
    "    denselayer_activation = 'linear'#'linear' #None\n",
    "    output_activation = 'linear'#'linear' #'linear'\n",
    "    input_layer = layers.Input(shape = (1, lag,), name = \"Input-Layer\")\n",
    "#     multi_head_attention_layer = TCN(return_sequences=True,dilations=[1, 2, 4, 8,16])(input_layer)\n",
    "    multi_head_attention_layer = MultiHeadAttention(\n",
    "    num_heads = lag,\n",
    "    key_dim = 1,\n",
    "    name=\"Main-Layer\")(input_layer,input_layer,input_layer)\n",
    "    glm=keras.layers.GlobalMaxPool1D()(multi_head_attention_layer)\n",
    "\n",
    "    # LSTM_layer2 = layers.LSTM(units=50,activation=None,return_sequences=True)(LSTM_layer)\n",
    "\n",
    "    flatten_layer1 = keras.layers.Flatten(name=\"Flatten-Layer\")(glm)\n",
    "    flatInput=keras.layers.Flatten(name=\"Flatten-Layer2\")(input_layer)\n",
    "    concat=keras.layers.concatenate([flatten_layer1,flatInput])\n",
    "\n",
    "    dense_layer1 = Dense(\n",
    "    dense_neuron,\n",
    "    activation = denselayer_activation,\n",
    "    name = \"Fully-Connected-Layer\")(concat)\n",
    "    drp=keras.layers.Dropout(0)(dense_layer1)\n",
    "\n",
    "    dense_layer2 = Dense(\n",
    "    look_forward,\n",
    "    activation = None,\n",
    "    name = \"Output-Layer\")(drp)\n",
    "#     drp=keras.layers.Dropout(0.1)(dense_layer1)\n",
    "\n",
    "\n",
    "\n",
    "        # Create Model\n",
    "    model = Model(inputs = [input_layer], outputs = dense_layer2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e2eb554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.808729Z",
     "iopub.status.busy": "2024-10-09T17:01:12.808350Z",
     "iopub.status.idle": "2024-10-09T17:01:12.817192Z",
     "shell.execute_reply": "2024-10-09T17:01:12.815906Z"
    },
    "papermill": {
     "duration": 0.03149,
     "end_time": "2024-10-09T17:01:12.819528",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.788038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def creat_model_tcn(lag,dense_neuron,look_forward):\n",
    "    denselayer_activation = 'linear' #None\n",
    "    output_activation = 'linear' #'linear'\n",
    "    input_layer = layers.Input(shape = (1, lag,), name = \"Input-Layer\")\n",
    "    multi_head_attention_layer = TCN(return_sequences=True,dilations=[1, 2, 4, 8])(input_layer)\n",
    "    conv = keras.layers.Conv1D(64,\n",
    "                      strides=2,\n",
    "                      kernel_size=4,\n",
    "                      activation=None,\n",
    "                      padding=\"same\",)(multi_head_attention_layer)#multi_head_attention_layer\n",
    "    conv2 = keras.layers.Conv1D(16,\n",
    "                      strides=2,\n",
    "                      kernel_size=4,\n",
    "                      activation=None,\n",
    "                      padding=\"same\",)(conv)\n",
    "    flatten_layer2=keras.layers.Flatten(name=\"Flatten-Layer2\")(conv2)\n",
    "    # concat=keras.layers.concatenate([flatten_layer1,flatten_layer2])\n",
    "\n",
    "    dense_layer1 = Dense(\n",
    "            look_forward,\n",
    "            activation = denselayer_activation,\n",
    "            name = \"Fully-Connected-Layer\")(flatten_layer2)\n",
    "\n",
    "    dense_layer2 = Dense(\n",
    "            look_forward,\n",
    "            activation = None,\n",
    "            name = \"Output-Layer\")(dense_layer1)\n",
    "\n",
    "        # Create Model\n",
    "    model = Model(inputs = [input_layer], outputs = dense_layer2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e371d462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.863602Z",
     "iopub.status.busy": "2024-10-09T17:01:12.863175Z",
     "iopub.status.idle": "2024-10-09T17:01:12.867963Z",
     "shell.execute_reply": "2024-10-09T17:01:12.866828Z"
    },
    "papermill": {
     "duration": 0.032283,
     "end_time": "2024-10-09T17:01:12.870222",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.837939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  callback = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "# history = model.fit([trainX], trainY, validation_data=([valX, valY]),\n",
    "#                     verbose = 0,\n",
    "#                     batch_size = batch_size, epochs=epochs,callbacks=[callback]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acb0cc75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:12.909328Z",
     "iopub.status.busy": "2024-10-09T17:01:12.908969Z",
     "iopub.status.idle": "2024-10-09T17:01:12.929128Z",
     "shell.execute_reply": "2024-10-09T17:01:12.928004Z"
    },
    "papermill": {
     "duration": 0.042535,
     "end_time": "2024-10-09T17:01:12.931430",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.888895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_model_test(dataset, data_means, dataset_seasonal, dataset_name, cluster_lable, lag, look_forward, sample_overlap, batch_size, epochs, learning_rate, suilin_smape, dataset_path,frequency, use_saved_model = False, save_trained_model = False):\n",
    "    print(\"len dataset\",len(dataset))\n",
    "    # Initialize Look Forward & Back\n",
    "    look_back=lag\n",
    "    calculations_method = 'per_series' # single_value | per_series\n",
    "    # Loop Trough Clusters\n",
    "    pre_start_time = time.time()\n",
    "#     trainX, valX, testX, trainY, valY, testY, test_means, val_means, val_seasonal, test_seasonal,test_seasonal2 = all_pre_process(dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency)\n",
    "    trainX, valX, testX, trainY, valY, testY, test_means, val_means, val_seasonal, test_seasonal,test_seasonal2 = all_pre_process_aug(dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency,dataset_name)\n",
    "    pre_end_time = time.time()\n",
    "    pre_execution_time = pre_end_time - pre_start_time\n",
    "    print(f\"Total Preprocessing time Execution time dataset {dataset_name}: {pre_execution_time} seconds\")\n",
    "\n",
    "\n",
    "    # \n",
    "\n",
    "    if use_saved_model == True:\n",
    "        if os.path.exists(dataset_path + '/' + dataset_name + '-model-cluster-' + str(cluster_lable)) == True:\n",
    "            model = keras.models.load_model(dataset_path + '/' + dataset_name  + '-model-cluster-' + str(cluster_lable))\n",
    "\n",
    "            val_prediction_results = model.predict([valX],batch_size=16, verbose=0)\n",
    "\n",
    "            val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "            val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "            ######################################################\n",
    "            test_prediction_results = model.predict([testX],batch_size=16, verbose=0)\n",
    "\n",
    "            test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "            test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "        else:\n",
    "            use_saved_model = False\n",
    "            save_trained_model = True\n",
    "\n",
    "    # Train Model From Scratch\n",
    "    if use_saved_model == False:\n",
    "\n",
    "        dense_neuron = 100 ###JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ\n",
    "        denselayer_activation = 'linear' #None\n",
    "        output_activation = 'linear' #'linear'\n",
    "\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"lag\", lag)\n",
    "        print(\"look_forward\", look_forward)\n",
    "        print(\"sample overlap\", sample_overlap)\n",
    "        print(\"trainshape\", trainX.shape)\n",
    "        print(\"valshape\", valX.shape)\n",
    "        print(\"testshape\", testX.shape)\n",
    "        print(learning_rate, dense_neuron, denselayer_activation, output_activation)\n",
    "\n",
    "\n",
    "\n",
    "        validation_loss=[]\n",
    "        test_loss=[]\n",
    "        iter = 1\n",
    "        train_start_time = time.time()                              \n",
    "        for j in range(iter):\n",
    "            model=creat_model_tcn(lag,dense_neuron,look_forward)\n",
    "         \n",
    "            # Optimizer\n",
    "            opt=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "            # Compile Model\n",
    "            model.compile(loss=\"mse\",optimizer=opt,metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "            for k in range(epochs): #epochs\n",
    "                history = model.fit([trainX], trainY, validation_data=([valX, valY]),verbose = 0,\n",
    "                                    batch_size = batch_size).history\n",
    "\n",
    "                val_prediction_results = model.predict([valX],batch_size=16, verbose=0)\n",
    "\n",
    "                val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "                val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                ######################################################\n",
    "                test_prediction_results = model.predict([testX],batch_size=16, verbose=0)\n",
    "\n",
    "                test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "                test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                validation_loss.append(np.mean(val_RMSE))\n",
    "                test_loss.append(np.mean(test_RMSE))\n",
    "\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "            # Save model to a file if wanted\n",
    "            if save_trained_model == True:\n",
    "                model.save(dataset_path + '/' + dataset_name + '-model-cluster-' + str(cluster_lable))\n",
    "\n",
    "            model=None\n",
    "            del history\n",
    "        train_end_time = time.time()\n",
    "    train_execution_time = train_end_time - train_start_time\n",
    "    print(f\"Total Training model time Execution time dataset {dataset_name}: {train_execution_time} seconds\")\n",
    "    #############################################\n",
    "    #############################################\n",
    "    print(test_seasonal.shape)\n",
    "    print(\"ddddddddddddddddd\")\n",
    "    print(test_seasonal2.shape)\n",
    "    rescaled_valY = rescale_data_to_main_value(valY, val_means, val_seasonal)\n",
    "#     print(\" rescaled_valY\", rescaled_valY)\n",
    "    rescaled_val_prediction_results = rescale_data_to_main_value(val_prediction_results, val_means,val_seasonal) #####\n",
    "    val_SMAPE = smape(rescaled_valY, rescaled_val_prediction_results, calculations_method, suilin_smape)\n",
    "    val_RMSE = root_mean_squared_error(rescaled_valY, rescaled_val_prediction_results, calculations_method)\n",
    "    # print(\"test seasonal\",test_seasonal)\n",
    "    # print(\"val\",val_seasonal)\n",
    "    # print(\"val[-1]\",val_seasonal[-1])\n",
    "    rescaled_testY = rescale_data_to_main_value(testY, test_means, test_seasonal)\n",
    "#     print(\" rescaled_testY\", rescaled_testY)\n",
    "\n",
    "    rescaled_trainY = rescale_data_to_main_value(dataset, data_means, dataset_seasonal)\n",
    "    val_MASE = mase_val(rescaled_valY,rescaled_val_prediction_results,rescaled_trainY,frequency)\n",
    "    # print(\"via resc\",test_seasonal)\n",
    "    rescaled_test_prediction_results = rescale_data_to_main_value(test_prediction_results, test_means,test_seasonal2 ) ###,test_seasonal\n",
    "    test_MASE = mase(rescaled_testY,rescaled_test_prediction_results,rescaled_trainY,frequency)\n",
    "    test_SMAPE = smape(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "    test_RMSE = root_mean_squared_error(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"TestY\",rescaled_testY)\n",
    "    # print( \"Prediction\",rescaled_test_prediction_results)\n",
    "\n",
    "\n",
    "    results = {\n",
    "            'val_SMAPE': val_SMAPE,\n",
    "            'val_RMSE': val_RMSE,\n",
    "            'val_MASE': val_MASE,\n",
    "            'test_SMAPE': test_SMAPE,\n",
    "            'test_RMSE': test_RMSE,\n",
    "            'test_MASE': test_MASE\n",
    "        }\n",
    "    #############################################\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78051176",
   "metadata": {
    "papermill": {
     "duration": 0.022145,
     "end_time": "2024-10-09T17:01:12.971711",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.949566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6b7348a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.019245Z",
     "iopub.status.busy": "2024-10-09T17:01:13.018051Z",
     "iopub.status.idle": "2024-10-09T17:01:13.026031Z",
     "shell.execute_reply": "2024-10-09T17:01:13.024841Z"
    },
    "papermill": {
     "duration": 0.034435,
     "end_time": "2024-10-09T17:01:13.028682",
     "exception": false,
     "start_time": "2024-10-09T17:01:12.994247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "clustering_results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27dbe708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.073384Z",
     "iopub.status.busy": "2024-10-09T17:01:13.072537Z",
     "iopub.status.idle": "2024-10-09T17:01:13.083118Z",
     "shell.execute_reply": "2024-10-09T17:01:13.081865Z"
    },
    "papermill": {
     "duration": 0.033309,
     "end_time": "2024-10-09T17:01:13.085493",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.052184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def cluster_series(features, number_of_clusters=2):\n",
    "    global clustering_results\n",
    "    if len(clustering_results)==0:\n",
    "#         clustered = KMedoids(n_clusters=number_of_clusters,init='k-medoids++',random_state=0).fit(features) # Kmedoids\n",
    "#         print(\"Kmedoids\")\n",
    "#         clustered = KMeans(n_clusters=number_of_clusters,init='k-means++', random_state=0).fit(features) # Kmeans\n",
    "#         print(\"Kmeans\")\n",
    "# #         clustering_results=clustered.labels_\n",
    "        cluster_start_time = time.time()\n",
    "        clustered = SpectralClustering(n_clusters=number_of_clusters,assign_labels = 'discretize',\n",
    "                                      random_state = 0).fit(features)\n",
    "        print(\"Spectral clustering\")\n",
    "        clustering_results=clustered.labels_\n",
    "\n",
    "        cluster_end_time = time.time()\n",
    "        culster_execution_time = cluster_end_time - cluster_start_time\n",
    "        print(f\"Clustering time Execution time dataset {dataset_name}: {culster_execution_time} seconds\")\n",
    "    # clustered = KMeans(n_clusters=number_of_clusters, random_state=0).fit(features) # Kmeans\n",
    "    # print(\"Kmeans\")\n",
    "\n",
    "        print('silhouette_score -------->', silhouette_score(features, clustered.labels_))\n",
    "        return clustered.labels_\n",
    "    else:\n",
    "        print('Using Saved clustering_results' )\n",
    "        return clustering_results\n",
    "        \n",
    "\n",
    "def stl_decomposition(dataset, frequency):\n",
    "    seasonal = []\n",
    "    trend = []\n",
    "    for index in range(len(dataset)):\n",
    "        if frequency != None:\n",
    "            stl = STL(dataset[index], frequency, \"periodic\")\n",
    "\n",
    "            seasonal.append(stl.seasonal)\n",
    "            trend.append(stl.trend)\n",
    "            dataset[index] = dataset[index] - stl.seasonal\n",
    "        else:\n",
    "            seasonal.append(np.zeros((dataset[index].shape)))\n",
    "            trend.append(np.zeros((dataset[index].shape)))\n",
    "\n",
    "    return dataset, np.array(seasonal), np.array(trend)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2324e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.124623Z",
     "iopub.status.busy": "2024-10-09T17:01:13.124186Z",
     "iopub.status.idle": "2024-10-09T17:01:13.146623Z",
     "shell.execute_reply": "2024-10-09T17:01:13.145477Z"
    },
    "papermill": {
     "duration": 0.044951,
     "end_time": "2024-10-09T17:01:13.149022",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.104071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_local_models(dataset_name, number_of_clusters=2, AEName='LSTM', Dim=8, epochs = 20, batch =20, use_saved_model = False, save_trained_model = False, run=1):\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print('dataset: ', dataset_name)\n",
    "    batch_size = batch\n",
    "    epochs = epochs\n",
    "    # Prepare & Read Data\n",
    "    dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency = get_dataset_params(dataset_name)\n",
    "\n",
    "    # Normalize Data\n",
    "    dataset, data_means = normalize_dataset(dataset, look_forward)\n",
    "\n",
    "    dataset, seasonal, trend = stl_decomposition2(dataset, frequency,look_forward)\n",
    "\n",
    "    # Normalize Features\n",
    "    features = normalize_feature_vectors(features)\n",
    "\n",
    "    # Cluster Series Based On Feature Vectors (Feature Based Clustering)\n",
    "    if number_of_clusters == 1:\n",
    "        clusters = np.zeros(len(features))\n",
    "    else:\n",
    "        clusters = cluster_series(features, number_of_clusters)\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    results = {\n",
    "        'val_SMAPE': np.array([]),\n",
    "        'val_RMSE': np.array([]),\n",
    "        'val_MASE': np.array([]),\n",
    "        'test_SMAPE': np.array([]),\n",
    "        'test_RMSE': np.array([]),\n",
    "        'test_MASE': np.array([])\n",
    "    }\n",
    "\n",
    "    # Loop Trough Clusters\n",
    "    start_time = time.time()\n",
    "    for cluster_lable in range(number_of_clusters):\n",
    "        idx = [x for x in range(len(clusters)) if clusters[x] == cluster_lable]\n",
    "        cluster_dataset = np.array(dataset)[idx]\n",
    "        cluster_dataset_means = data_means[idx]\n",
    "        cluster_dataset_seasonal = seasonal[idx]\n",
    "\n",
    "        result = run_model_test(cluster_dataset, cluster_dataset_means, cluster_dataset_seasonal, dataset_name, cluster_lable, lag, look_forward, sample_overlap, batch_size, epochs, learning_rate, suilin_smape, dataset_path, frequency, use_saved_model, save_trained_model)\n",
    "\n",
    "        results = {\n",
    "            'val_SMAPE': np.concatenate((results['val_SMAPE'], result['val_SMAPE'])),\n",
    "            'val_RMSE': np.concatenate((results['val_RMSE'], result['val_RMSE'])),\n",
    "            'val_MASE': np.concatenate((results['val_MASE'], result['val_MASE'])),\n",
    "            'test_SMAPE': np.concatenate((results['test_SMAPE'], result['test_SMAPE'])),\n",
    "            'test_RMSE': np.concatenate((results['test_RMSE'], result['test_RMSE'])),\n",
    "            'test_MASE': np.concatenate((results['test_MASE'], result['test_MASE']))\n",
    "        }\n",
    "        \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Total Cluster Training time Execution time dataset {dataset_name}: {execution_time} seconds\")\n",
    "    # print(\"test smape\",results['test_SMAPE'])\n",
    "    # Print Results Table\n",
    "    filename_val_smape = \"val_SMAPE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_val_mase = \"val_MASE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_val_rmse = \"val_RMSE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    \n",
    "    filename_test_smape = \"test_SMAPE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_test_mase = \"test_MASE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "\n",
    "    filename_test_rmse = \"test_RMSE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "\n",
    "    # Save the NumPy array to the file\n",
    "    np.save(filename_val_smape, results['val_SMAPE'])\n",
    "    np.save(filename_val_rmse, results['val_RMSE'])\n",
    "    np.save(filename_val_mase, results['val_MASE'])\n",
    "\n",
    "    np.save(filename_test_smape, results['test_SMAPE'])\n",
    "    np.save(filename_test_rmse, results['test_RMSE'])\n",
    "    np.save(filename_test_mase, results['test_MASE'])\n",
    "    t = Texttable()\n",
    "    print('\\n\\n#------------------------------------Scaled------------------------------------#')\n",
    "    t.add_rows([\n",
    "        ['Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE', 'Mean MASE'],\n",
    "        ['Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE']), np.mean(results['val_MASE'])],\n",
    "        ['Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE']), np.mean(results['test_MASE'])]\n",
    "    ])\n",
    "    \n",
    "    print(t.draw())\n",
    "    t = Texttable()\n",
    "    t.add_rows([\n",
    "        ['dataset Name', 'Run', 'N.Epochs', 'Batch Size', 'Auto Encoder', 'Latent Dimension'],\n",
    "        [dataset_name, run, epochs,batch_size, AEName, Dim]\n",
    "    ])\n",
    "    print(t.draw())\n",
    "    initial_data = [\n",
    "    ['dataset Name', 'Run', 'N.Epochs', 'Batch Size', 'Auto Encoder', 'Latent Dimension', 'Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE', 'Mean MASE','Median MASE'],\n",
    "    [dataset_name, run, epochs, batch_size, AEName, Dim, 'Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE']), np.mean(results['val_MASE']),np.median(results['val_MASE'])],\n",
    "    [dataset_name, run, epochs, batch_size, AEName, Dim, 'Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE']), np.mean(results['test_MASE']),np.median(results['test_MASE'])]\n",
    "    ]# Write the header and initial data to the CSV file\n",
    "\n",
    "    return results, initial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddd1e1",
   "metadata": {
    "papermill": {
     "duration": 0.018649,
     "end_time": "2024-10-09T17:01:13.185902",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.167253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07036a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.225515Z",
     "iopub.status.busy": "2024-10-09T17:01:13.225069Z",
     "iopub.status.idle": "2024-10-09T17:01:13.242432Z",
     "shell.execute_reply": "2024-10-09T17:01:13.241298Z"
    },
    "papermill": {
     "duration": 0.040134,
     "end_time": "2024-10-09T17:01:13.244799",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.204665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "five_run= {\n",
    "        'mean_val_SMAPE_5': np.array([]),\n",
    "        'mean_val_RMSE_5': np.array([]),\n",
    "        'mean_test_SMAPE_5': np.array([]),\n",
    "        'mean_test_RMSE_5': np.array([]),\n",
    "        'median_val_SMAPE_5': np.array([]),\n",
    "        'median_val_RMSE_5': np.array([]),\n",
    "        'median_test_SMAPE_5': np.array([]),\n",
    "        'median_test_RMSE_5': np.array([])\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# ##############################\n",
    "def cif(AEName=\"LSTM\", Dim=16, run=1):\n",
    "#     ds_names = [AEName, [5,15,25,35,50], [3,6,10, 20,40,60,80,100], Dim]\n",
    "    ds_names = [AEName, [50], [6], Dim]\n",
    "\n",
    "    file_mode = 'w'\n",
    "    csv_file = AEName+\"_\"+str(Dim)+\".csv\"\n",
    "    file_list.append(csv_file)\n",
    "    if os.path.exists(csv_file):\n",
    "        file_mode = 'a'\n",
    "    else:\n",
    "        file_mode = 'w'\n",
    "    with open(csv_file, mode=file_mode, newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for epoch in ds_names[1]:\n",
    "            for batch in ds_names[2]:\n",
    "                    cif12_results, initial_data_cif12 = run_local_models(dataset_name = 'cif-12', number_of_clusters = 2, AEName=ds_names[0], Dim=ds_names[3], epochs = epoch, batch = batch, use_saved_model = False, save_trained_model = False, run=run)\n",
    "                    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "                    cif6_results, initial_data_cif06 = run_local_models(dataset_name = 'cif-6', number_of_clusters = 1, AEName=ds_names[0], Dim=ds_names[3], epochs = epoch, batch = batch, use_saved_model = False, save_trained_model = False, run=run)\n",
    "                    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "                    print('\\n--------------------------------------Mixed Results---------------------------------------------\\n')\n",
    "                    results = {\n",
    "                        'val_SMAPE': np.concatenate((cif12_results['val_SMAPE'], cif6_results['val_SMAPE'])),\n",
    "                        'val_RMSE': np.concatenate((cif12_results['val_RMSE'], cif6_results['val_RMSE'])),\n",
    "                        'val_MASE': np.concatenate((cif12_results['val_MASE'], cif6_results['val_MASE'])),\n",
    "                        'test_SMAPE': np.concatenate((cif12_results['test_SMAPE'], cif6_results['test_SMAPE'])),\n",
    "                        'test_RMSE': np.concatenate((cif12_results['test_RMSE'], cif6_results['test_RMSE'])),\n",
    "                        'test_MASE': np.concatenate((cif12_results['test_MASE'], cif6_results['test_MASE']))\n",
    "                    }\n",
    "\n",
    "                    t = Texttable()\n",
    "                    print('\\n\\n#------------------------------------Scaled------------------------------------#')\n",
    "                    t.add_rows([\n",
    "                        ['Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE', 'Mean MASE','Meadian MASE'],\n",
    "                        ['Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE']),np.mean(results['val_MASE']), np.median(results['val_MASE'])],\n",
    "                        ['Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE']),np.mean(results['test_MASE']), np.median(results['test_MASE'])]\n",
    "                    ])\n",
    "                    initial_data = []\n",
    "                    \n",
    "                    initial_data = initial_data_cif12 + initial_data_cif06\n",
    "                    initial_data.extend(\n",
    "                        [\n",
    "                        ['dataset Name', 'Run', 'N.Epochs', 'Batch Size', 'Auto Encoder', 'Latent Dimension', 'Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE','Mean MASE', 'Median MASE'],\n",
    "                        ['Mixed', run, epoch, batch, AEName, Dim, 'Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE']),np.mean(results['val_MASE']), np.median(results['val_MASE'])],\n",
    "                        ['Mixed', run, epoch, batch, AEName, Dim, 'Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE']),np.mean(results['test_MASE']), np.median(results['test_MASE'])]\n",
    "                        ]\n",
    "                    )\n",
    "                    writer.writerows(initial_data)\n",
    "                    print(t.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "905bfe22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.284732Z",
     "iopub.status.busy": "2024-10-09T17:01:13.284329Z",
     "iopub.status.idle": "2024-10-09T17:01:13.289414Z",
     "shell.execute_reply": "2024-10-09T17:01:13.288160Z"
    },
    "papermill": {
     "duration": 0.027522,
     "end_time": "2024-10-09T17:01:13.291933",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.264411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# /kaggle/input/nn5-autoformer-testing-features/Features_nn5_Autoformer_max_mean_8_label_mv41_raw_head8_e10_mvg25.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d5cf8",
   "metadata": {
    "papermill": {
     "duration": 0.018349,
     "end_time": "2024-10-09T17:01:13.328919",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.310570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1382e67e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.374748Z",
     "iopub.status.busy": "2024-10-09T17:01:13.374349Z",
     "iopub.status.idle": "2024-10-09T17:01:13.379569Z",
     "shell.execute_reply": "2024-10-09T17:01:13.378635Z"
    },
    "papermill": {
     "duration": 0.030189,
     "end_time": "2024-10-09T17:01:13.381849",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.351660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba8184",
   "metadata": {
    "papermill": {
     "duration": 0.018015,
     "end_time": "2024-10-09T17:01:13.418572",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.400557",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb76ed24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.456436Z",
     "iopub.status.busy": "2024-10-09T17:01:13.456044Z",
     "iopub.status.idle": "2024-10-09T17:01:13.462884Z",
     "shell.execute_reply": "2024-10-09T17:01:13.461841Z"
    },
    "papermill": {
     "duration": 0.028381,
     "end_time": "2024-10-09T17:01:13.465184",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.436803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mdll\n",
    "num_cluster2={'tourism':1,'cif-12':1,'m3-demo':1, 'm3-finance':1, 'm3-industry':1, 'm3-micro':1, 'm3-macro':1, 'm3-other':1, 'nn5':1, 'hospital':1 }\n",
    "def get_param2():\n",
    "    dataset_name='hospital'#'m3-demo'\n",
    "    num_clust=num_cluster2[dataset_name]\n",
    "#     num_clust=1\n",
    "\n",
    "    batch=[10]\n",
    "    epoch=[10]# 5,10,15\n",
    "\n",
    "#     epoch=[5,10,15,25,35,50]# 5,10,15\n",
    "\n",
    "#     batch=[2,3,4,5,6,7,10]\n",
    "#     epoch=[5,10,20,30]\n",
    "#     batch=[3]\n",
    "#     epoch=[15]\n",
    "#     batch=[10,20,30,35,40,60,80]\n",
    "#     epoch=[5,10,15,20,30,40,50]# 5,10,15\n",
    "\n",
    "    mdl=dataset_name+\"LSTM_8\"\n",
    "    \n",
    "    return dataset_name,num_clust,batch, epoch,mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4495ed7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.504591Z",
     "iopub.status.busy": "2024-10-09T17:01:13.504169Z",
     "iopub.status.idle": "2024-10-09T17:01:13.516856Z",
     "shell.execute_reply": "2024-10-09T17:01:13.515787Z"
    },
    "papermill": {
     "duration": 0.035229,
     "end_time": "2024-10-09T17:01:13.519343",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.484114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#USED FOR ALL DATASET EXCEPT CIF CIF CIF CIF CIF CIF CIF CIF\n",
    "def m3(AEName=\"\", Dim=0, run=1):\n",
    "#     ds_names = [AEName, [5, 15, 25, 35, 50], [20, 40, 60, 80, 100], Dim]\n",
    "#     ds_names = [AEName, [1,50], [20,10000], Dim]\n",
    "    dataset_name,num_cluster,batch, epoch,mdl=get_param2()\n",
    "    ds_names = [AEName, epoch, batch, Dim]\n",
    "    file_mode = 'w'\n",
    "    csv_file = AEName+\"_\"+str(Dim)+\".csv\"\n",
    "   \n",
    "    file_list.append(csv_file)\n",
    "    file_mode = 'w'\n",
    "\n",
    "\n",
    "    if os.path.exists(csv_file):\n",
    "        file_mode = 'a'\n",
    "    else:\n",
    "        file_mode = 'w'\n",
    "    with open(csv_file, mode=file_mode, newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for epoch in ds_names[1]:\n",
    "            for batch in ds_names[2]:\n",
    "                if epoch==500 and batch==200:\n",
    "                    continue\n",
    "                else:\n",
    "                    nn5_results, initial_data_nn5 = run_local_models(dataset_name =dataset_name , number_of_clusters = num_cluster, AEName=ds_names[0], Dim=ds_names[3], epochs = epoch, batch = batch, use_saved_model = False, save_trained_model = False, run=run)\n",
    "                    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "                    results = {\n",
    "                        'val_SMAPE': nn5_results['val_SMAPE'],\n",
    "                        'val_RMSE': nn5_results['val_RMSE'],\n",
    "                        'val_MASE': nn5_results['val_MASE'],\n",
    "                        'test_SMAPE': nn5_results['test_SMAPE'],\n",
    "                        'test_RMSE':nn5_results['test_RMSE'] ,\n",
    "                        'test_MASE':nn5_results['test_MASE']\n",
    "                    }\n",
    "                    t = Texttable()\n",
    "                    print('\\n\\n#------------------------------------Scaled------------------------------------#')\n",
    "                    t.add_rows([\n",
    "                        ['Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE','Mean MASE', 'Median MASE'],\n",
    "                        ['Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE']),np.mean(results['val_MASE']), np.median(results['val_MASE'])],\n",
    "                        ['Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE']),np.mean(results['test_MASE']), np.median(results['test_MASE'])]\n",
    "                    ])\n",
    "                    initial_data = []\n",
    "\n",
    "                    initial_data = initial_data_nn5 \n",
    "\n",
    "                    writer.writerows(initial_data)\n",
    "                    print(t.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c67ad108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.557765Z",
     "iopub.status.busy": "2024-10-09T17:01:13.557375Z",
     "iopub.status.idle": "2024-10-09T17:01:13.562174Z",
     "shell.execute_reply": "2024-10-09T17:01:13.561073Z"
    },
    "papermill": {
     "duration": 0.026635,
     "end_time": "2024-10-09T17:01:13.564531",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.537896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NOTTTTTTTTTTTTTfor i in range(2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f46ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:01:13.603610Z",
     "iopub.status.busy": "2024-10-09T17:01:13.602616Z",
     "iopub.status.idle": "2024-10-09T17:04:55.075202Z",
     "shell.execute_reply": "2024-10-09T17:04:55.073818Z"
    },
    "papermill": {
     "duration": 221.494809,
     "end_time": "2024-10-09T17:04:55.077748",
     "exception": false,
     "start_time": "2024-10-09T17:01:13.582939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    dataset_name,num_cluster,batch, epoch,mdl=get_param2()\n",
    "    dim=0\n",
    "    m3(mdl,dim,run=i)\n",
    "#     cif(mdl,dim,run=i)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# t1 = Texttable()\n",
    "# print('\\n\\n#------------------------------------Scaled------------------------------------#')\n",
    "# t1.add_rows([\n",
    "#     ['Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE'],\n",
    "#     ['Validate', five_run['mean_val_SMAPE_5'], five_run['mean_val_RMSE_5'], five_run['median_val_SMAPE_5'], five_run['median_val_RMSE_5']],\n",
    "#     ['Test', five_run['mean_test_SMAPE_5'], five_run['mean_test_RMSE_5'], five_run['median_test_SMAPE_5'], five_run['median_test_RMSE_5']]\n",
    "# ])\n",
    "# print(t1.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94487bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:04:55.116634Z",
     "iopub.status.busy": "2024-10-09T17:04:55.116239Z",
     "iopub.status.idle": "2024-10-09T17:04:55.123842Z",
     "shell.execute_reply": "2024-10-09T17:04:55.122750Z"
    },
    "papermill": {
     "duration": 0.029695,
     "end_time": "2024-10-09T17:04:55.126200",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.096505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61449f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:04:55.165705Z",
     "iopub.status.busy": "2024-10-09T17:04:55.165295Z",
     "iopub.status.idle": "2024-10-09T17:04:55.218704Z",
     "shell.execute_reply": "2024-10-09T17:04:55.217559Z"
    },
    "papermill": {
     "duration": 0.076628,
     "end_time": "2024-10-09T17:04:55.221481",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.144853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "directory_path = '/kaggle/working/'\n",
    "# csv_file='results_Hyndman_0.csv'\n",
    "# file_list = os.listdir(directory_path)\n",
    "\n",
    "# csv_files = [csv_file]\n",
    "# file_list=['LSTM_results_hospital_LSTM_8_32.csv']\n",
    "# Iterate through CSV files and process them\n",
    "for csv_file in file_list[-1:]:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    print(df)\n",
    "\n",
    "    results = []\n",
    "\n",
    "#     epochs = ['5', '15', '25', '35', '50']\n",
    "#     batchs = ['20', '40', '60', '80', '100']\n",
    "    dataset_name,num_cluster,batchs, epochs,mdl=get_param2()\n",
    "    print(\"hhhhhhhhhhhhhhh\")\n",
    "\n",
    "  \n",
    "\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            if epoch==500 and batch==200:\n",
    "                continue\n",
    "            validate_df = df[(df[2] == str(epoch)) & (df[3] == str(batch)) & (df[6] == 'Validate')]\n",
    "            test_df = df[(df[2] == str(epoch)) & (df[3] == str(batch)) & (df[6] == 'Test')]\n",
    "            validate_df.loc[:, [7,8, 9, 10,11,12]] = validate_df.loc[:, [7,8, 9, 10,11,12]].apply(pd.to_numeric, errors='coerce')\n",
    "            test_df.loc[:, [7,8, 9, 10,11,12]] = test_df.loc[:, [7,8, 9, 10,11,12]].apply(pd.to_numeric, errors='coerce')\n",
    "#             print(validate_df)\n",
    "            # Calculate mean for the columns\n",
    "            validate_mean = validate_df[[7,8, 9, 10,11,12]].mean()\n",
    "#             print(\"validate_mean\",validate_mean)\n",
    "            test_mean = test_df[[7,8, 9, 10,11,12]].mean()\n",
    "\n",
    "            # Append results along with epoch and batch information to the results list\n",
    "            results.append({\n",
    "                'Epoch': epoch,\n",
    "                'Batch': batch,\n",
    "                'Validate_Mean_sMAPE': validate_mean[7],\n",
    "                'Validate_Median_sMAPE': validate_mean[8],\n",
    "                'Validate_Mean_RMSE': validate_mean[9],\n",
    "                'Validate_Median_RMSE': validate_mean[10],\n",
    "                'Validate_Mean_MASE': validate_mean[11],\n",
    "                'Validate_Median_MASE': validate_mean[12],\n",
    "                \n",
    "                'Test_Mean_Mean_sMAPE': test_mean[7],\n",
    "                'Test_Mean_Median_sMAPE': test_mean[8],\n",
    "                'Test_Mean_Mean_RMSE': test_mean[9],\n",
    "                'Test_Mean_Median_RMSE': test_mean[10],\n",
    "                'Test_Mean_Mean_MASE': test_mean[11],\n",
    "                'Test_Mean_Median_MASE': test_mean[12]\n",
    "            })\n",
    "    columns = [\n",
    "                'Epoch',\n",
    "                'Batch',\n",
    "                'Validate_Mean_sMAPE',\n",
    "                'Validate_Median_sMAPE',\n",
    "                'Validate_Mean_RMSE',\n",
    "                'Validate_Median_RMSE',\n",
    "                'Validate_Mean_MASE',\n",
    "                'Validate_Median_MASE',\n",
    "                'Test_Mean_Mean_sMAPE',\n",
    "                'Test_Mean_Median_sMAPE',\n",
    "                'Test_Mean_Mean_RMSE',\n",
    "                'Test_Mean_Median_RMSE',\n",
    "                'Test_Mean_Mean_MASE',\n",
    "                'Test_Mean_Median_MASE'\n",
    "    ]\n",
    "    result_df = pd.DataFrame(results, columns = columns)\n",
    "    result_df = result_df.sort_values(by='Validate_Mean_sMAPE', ascending=True)\n",
    "    result_df.to_csv(\"Average_Results_\"+csv_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3c5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:04:55.261865Z",
     "iopub.status.busy": "2024-10-09T17:04:55.261452Z",
     "iopub.status.idle": "2024-10-09T17:04:55.268667Z",
     "shell.execute_reply": "2024-10-09T17:04:55.267454Z"
    },
    "papermill": {
     "duration": 0.030179,
     "end_time": "2024-10-09T17:04:55.270988",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.240809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d7d7f",
   "metadata": {
    "papermill": {
     "duration": 0.018447,
     "end_time": "2024-10-09T17:04:55.309031",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.290584",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bc5ac1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T17:04:55.348923Z",
     "iopub.status.busy": "2024-10-09T17:04:55.348512Z",
     "iopub.status.idle": "2024-10-09T17:04:55.353106Z",
     "shell.execute_reply": "2024-10-09T17:04:55.352029Z"
    },
    "papermill": {
     "duration": 0.026737,
     "end_time": "2024-10-09T17:04:55.355239",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.328502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#shutil.make_archive(OUTPUT_NAME, 'zip', DIRECTORY_TO_ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63bceb1",
   "metadata": {
    "papermill": {
     "duration": 0.018786,
     "end_time": "2024-10-09T17:04:55.393243",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.374457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69141b9a",
   "metadata": {
    "papermill": {
     "duration": 0.018787,
     "end_time": "2024-10-09T17:04:55.430830",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.412043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb3d66",
   "metadata": {
    "papermill": {
     "duration": 0.018782,
     "end_time": "2024-10-09T17:04:55.469151",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.450369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9150103",
   "metadata": {
    "papermill": {
     "duration": 0.018437,
     "end_time": "2024-10-09T17:04:55.506492",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.488055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4599a3b",
   "metadata": {
    "papermill": {
     "duration": 0.018796,
     "end_time": "2024-10-09T17:04:55.544149",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.525353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44297966",
   "metadata": {
    "papermill": {
     "duration": 0.018777,
     "end_time": "2024-10-09T17:04:55.581706",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.562929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a0b99",
   "metadata": {
    "papermill": {
     "duration": 0.019149,
     "end_time": "2024-10-09T17:04:55.619591",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.600442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1461a",
   "metadata": {
    "papermill": {
     "duration": 0.018617,
     "end_time": "2024-10-09T17:04:55.657168",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.638551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c4361",
   "metadata": {
    "papermill": {
     "duration": 0.018573,
     "end_time": "2024-10-09T17:04:55.694872",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.676299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e79acf",
   "metadata": {
    "papermill": {
     "duration": 0.018478,
     "end_time": "2024-10-09T17:04:55.732099",
     "exception": false,
     "start_time": "2024-10-09T17:04:55.713621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3863893,
     "sourceId": 7579986,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3785054,
     "sourceId": 7651500,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4510828,
     "sourceId": 7722007,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4520850,
     "sourceId": 7735771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4624107,
     "sourceId": 7889633,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4422836,
     "sourceId": 7917614,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3813359,
     "sourceId": 7917616,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5020438,
     "sourceId": 9038754,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4454423,
     "sourceId": 9326749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5845880,
     "sourceId": 9586060,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 146439927,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146440926,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146442216,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146443044,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146550165,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146550185,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 147500857,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 147503812,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 157337232,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 157337836,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199643187,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199643209,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199643568,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199643925,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199644303,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199644662,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199645195,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199652202,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199652215,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 200139995,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 322.157632,
   "end_time": "2024-10-09T17:04:58.322520",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-09T16:59:36.164888",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
